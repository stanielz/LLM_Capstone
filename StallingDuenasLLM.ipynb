{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3f7109d-c067-40a4-bc00-fd067ff2fc69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "Device name: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.amp import GradScaler, autocast\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from torch.utils.data import Dataset\n",
    "import re\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Device name:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e983a4c-0559-4edf-8c50-c77f59103d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def get_sparse_causal_mask(seq_len, window_size, device):\n",
    "    \"\"\"\n",
    "    Generates a sparse causal mask for attention.\n",
    "    Each token at position i attends only to tokens in the range:\n",
    "       [max(0, i - window_size + 1), i]\n",
    "    All other positions are masked out.\n",
    "    \n",
    "    Returns a Boolean tensor of shape [seq_len, seq_len] where True indicates a masked position.\n",
    "    \"\"\"\n",
    "    # Causal (lower triangular) mask.\n",
    "    causal_mask = torch.tril(torch.ones(seq_len, seq_len, dtype=torch.bool, device=device))\n",
    "    # Sliding window mask: zero out positions too far in the past.\n",
    "    window_mask = torch.triu(torch.ones(seq_len, seq_len, dtype=torch.bool, device=device), diagonal=-(window_size - 1))\n",
    "    allowed = causal_mask & window_mask\n",
    "    return ~allowed  # True means masked out.\n",
    "\n",
    "\n",
    "def get_rotary_embeddings(seq_len, head_dim, device):\n",
    "    \"\"\"\n",
    "    Computes cosine and sine embeddings for ROPE.\n",
    "    head_dim is assumed to be even.\n",
    "    Returns tensors of shape [1, 1, seq_len, head_dim//2] for cos and sin.\n",
    "    \"\"\"\n",
    "    # Create inverse frequency vector\n",
    "    inv_freq = 1.0 / (10000 ** (torch.arange(0, head_dim, 2, device=device).float() / head_dim))\n",
    "    t = torch.arange(seq_len, device=device, dtype=torch.float32)\n",
    "    # Compute the outer product: [seq_len, head_dim//2]\n",
    "    freqs = torch.einsum(\"i,j->ij\", t, inv_freq)\n",
    "    cos = torch.cos(freqs).unsqueeze(0).unsqueeze(0)  # shape: [1, 1, seq_len, head_dim//2]\n",
    "    sin = torch.sin(freqs).unsqueeze(0).unsqueeze(0)  # shape: [1, 1, seq_len, head_dim//2]\n",
    "    return cos, sin\n",
    "\n",
    "def apply_rotary_pos_emb(x, cos, sin):\n",
    "    \"\"\"\n",
    "    Applies rotary positional embedding.\n",
    "    x: Tensor of shape [batch_size, num_heads, seq_len, head_dim] where head_dim is even.\n",
    "    Splits the head dimension into even and odd parts, rotates, and then interleaves them back.\n",
    "    \"\"\"\n",
    "    # Split into even and odd parts: each of shape [..., head_dim//2]\n",
    "    x_even = x[..., ::2]\n",
    "    x_odd = x[..., 1::2]\n",
    "    # Apply rotation: for each position, compute:\n",
    "    # new_even = x_even * cos - x_odd * sin\n",
    "    # new_odd  = x_even * sin + x_odd * cos\n",
    "    x_rotated_even = x_even * cos - x_odd * sin\n",
    "    x_rotated_odd  = x_even * sin + x_odd * cos\n",
    "    # Interleave the even and odd parts back together.\n",
    "    # One way is to stack along a new last dimension then flatten it.\n",
    "    x_out = torch.stack((x_rotated_even, x_rotated_odd), dim=-1).flatten(-2)\n",
    "    return x_out\n",
    "\n",
    "class HybridMultiheadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.0, sparse_window_size=63, causal_heads=0):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be divisible by the number of heads.\"\n",
    "        # For ROPE, we assume head_dim is even.\n",
    "        assert self.head_dim % 2 == 0, \"head_dim must be even for ROPE encoding.\"\n",
    "        self.causal_heads = causal_heads\n",
    "        self.sparse_window_size = sparse_window_size\n",
    "\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, query, key, value, key_padding_mask=None):\n",
    "        batch_size, seq_len, _ = query.size()\n",
    "\n",
    "        # Linear projections.\n",
    "        q = self.q_proj(query)\n",
    "        k = self.k_proj(key)\n",
    "        v = self.v_proj(value)\n",
    "\n",
    "        # Reshape to [batch_size, num_heads, seq_len, head_dim].\n",
    "        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        device = query.device\n",
    "        # --- ROPE: apply rotary positional encoding to q and k ---\n",
    "        cos, sin = get_rotary_embeddings(seq_len, self.head_dim, device)\n",
    "        q = apply_rotary_pos_emb(q, cos, sin)\n",
    "        k = apply_rotary_pos_emb(k, cos, sin)\n",
    "        # ----------------------------------------------------------\n",
    "\n",
    "        scale = math.sqrt(self.head_dim)\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / scale  # [batch_size, num_heads, seq_len, seq_len]\n",
    "\n",
    "        # Apply key padding mask if provided.\n",
    "        if key_padding_mask is not None:\n",
    "            kp_mask = key_padding_mask.unsqueeze(1).unsqueeze(2)  # [batch_size, 1, 1, seq_len]\n",
    "            scores = scores.masked_fill(kp_mask, float('-inf'))\n",
    "\n",
    "        # Apply causal mask to the first 'causal_heads'.\n",
    "        if self.causal_heads > 0:\n",
    "            causal_mask = torch.triu(torch.ones(seq_len, seq_len, dtype=torch.bool, device=device), diagonal=1)\n",
    "            causal_mask = causal_mask.unsqueeze(0).unsqueeze(0)  # [1, 1, seq_len, seq_len]\n",
    "            scores[:, :self.causal_heads] = scores[:, :self.causal_heads].masked_fill(causal_mask, float('-inf'))\n",
    "\n",
    "        # Apply sparse sliding-window mask to remaining heads.\n",
    "        num_sparse_heads = self.num_heads - self.causal_heads\n",
    "        if num_sparse_heads > 0:\n",
    "            sparse_mask = get_sparse_causal_mask(seq_len, self.sparse_window_size, device)\n",
    "            sparse_mask = sparse_mask.unsqueeze(0).unsqueeze(0)  # [1, 1, seq_len, seq_len]\n",
    "            scores[:, self.causal_heads:] = scores[:, self.causal_heads:].masked_fill(sparse_mask, float('-inf'))\n",
    "\n",
    "        # Check for rows that are entirely masked; replace them with zeros.\n",
    "        all_masked = (scores == float('-inf')).all(dim=-1, keepdim=True)\n",
    "        if all_masked.any():\n",
    "            scores = scores.masked_fill(all_masked, 0)\n",
    "\n",
    "        # Compute attention weights.\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = F.dropout(attn_weights, p=self.dropout, training=self.training)\n",
    "\n",
    "        # Compute attention output.\n",
    "        attn_output = torch.matmul(attn_weights, v)  # [batch_size, num_heads, seq_len, head_dim]\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)\n",
    "        output = self.out_proj(attn_output)\n",
    "\n",
    "        return output.to(query.dtype)\n",
    "\n",
    "# In your TransformerLM class, you no longer need to add a precomputed sinusoidal positional encoding.\n",
    "# Instead, you simply use token embeddings, and the rotary encoding will be applied in the attention layer.\n",
    "class TransformerLM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        pad_token_id,\n",
    "        embed_dim=512,\n",
    "        num_heads=8,\n",
    "        num_layers=6,\n",
    "        ff_dim=2048,\n",
    "        dropout_rate=0.1,\n",
    "        max_seq_len=250,\n",
    "        drop_path_rate=0.0,\n",
    "        sparse_window_size=None,  # Use an integer (e.g., 16 or 32) to use sparse attention.\n",
    "        causal_heads=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.pad_token_id = pad_token_id  # For building the padding_mask\n",
    "\n",
    "        # Token Embeddings\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "        # (Remove sinusoidal positional encoding here; ROPE is applied inside attention)\n",
    "        # self.positional_encoding = nn.Parameter(get_sinusoid_encoding(max_seq_len, embed_dim), requires_grad=False)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, ff_dim, dropout_rate, drop_path_rate,\n",
    "                             sparse_window_size=sparse_window_size, causal_heads=causal_heads)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.fc_out = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "        self.fc_out.weight = self.embedding.weight  # Weight Tying\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: [batch_size, seq_len] of token IDs.\n",
    "        \"\"\"\n",
    "        padding_mask = (x == self.pad_token_id)  # [batch_size, seq_len]\n",
    "        seq_len = x.size(1)\n",
    "\n",
    "        # Token embedding; note that we no longer add a fixed positional encoding.\n",
    "        x = self.embedding(x) * math.sqrt(self.embed_dim)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x, padding_mask)\n",
    "\n",
    "        logits = self.fc_out(x)  # [batch_size, seq_len, vocab_size]\n",
    "        return logits\n",
    "\n",
    "# Note: The rest of your code (e.g., TransformerBlock, LabelSmoothingCrossEntropy, etc.)\n",
    "# remains the same.\n",
    "\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate, drop_path_rate=0.0, sparse_window_size=0, causal_heads=None):\n",
    "        \"\"\"\n",
    "        :param embed_dim: Embedding dimension.\n",
    "        :param num_heads: Number of attention heads.\n",
    "        :param ff_dim: Hidden dimension in the feed-forward network.\n",
    "        :param dropout_rate: Dropout rate.\n",
    "        :param drop_path_rate: DropPath rate.\n",
    "        :param sparse_window_size: If set, uses sparse sliding-window attention for non-causal heads.\n",
    "        :param causal_heads: Number of heads to use full causal attention. If not provided, you can default to half the heads.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        # Set default: half the heads use full causal attention.\n",
    "        if causal_heads is None:\n",
    "            causal_heads = num_heads // 2\n",
    "        self.attn = HybridMultiheadAttention(\n",
    "            embed_dim,\n",
    "            num_heads,\n",
    "            dropout=dropout_rate,\n",
    "            causal_heads=causal_heads,\n",
    "            sparse_window_size=sparse_window_size\n",
    "        )\n",
    "        self.drop_path = nn.Dropout(drop_path_rate) if drop_path_rate > 0. else nn.Identity()\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(ff_dim, embed_dim),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        self.sparse_window_size = sparse_window_size\n",
    "\n",
    "    def forward(self, x, padding_mask=None):\n",
    "        # Pre-LayerNorm.\n",
    "        x_norm = self.norm1(x)  # [batch_size, seq_len, embed_dim]\n",
    "        # The custom attention module handles masking per head.\n",
    "        attn_out = self.attn(x_norm, x_norm, x_norm, key_padding_mask=padding_mask)\n",
    "        \n",
    "        # Residual connection with DropPath.\n",
    "        x = x + self.drop_path(attn_out)\n",
    "        \n",
    "        # Feed-Forward network.\n",
    "        x_norm = self.norm2(x)\n",
    "        ffn_out = self.ffn(x_norm)\n",
    "        x = x + self.drop_path(ffn_out)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    def __init__(self, ignore_index=-250, smoothing=0.0, reduction=\"mean\"):\n",
    "        \"\"\"\n",
    "        Constructor for the LabelSmoothingCrossEntropy module.\n",
    "        \n",
    "        :param smoothing: Label smoothing factor.\n",
    "        :param ignore_index: Specifies a target value that is ignored and does not contribute to the input gradient.\n",
    "        :param reduction: Specifies the reduction to apply to the output: 'mean' or 'sum'.\n",
    "        \"\"\"\n",
    "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
    "        self.ignore_index = ignore_index\n",
    "        self.smoothing = smoothing\n",
    "        self.reduction = reduction\n",
    "        self.confidence = 1.0 - smoothing\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        \"\"\"\n",
    "        Forward pass for label smoothing cross entropy.\n",
    "        \n",
    "        :param pred: Predictions (logits) [batch_size, num_classes].\n",
    "        :param target: Ground truth labels [batch_size].\n",
    "        :return: Smoothed cross entropy loss.\n",
    "        \"\"\"\n",
    "        log_pred = F.log_softmax(pred, dim=-1)\n",
    "\n",
    "        # If no smoothing is required, use the standard loss for efficiency.\n",
    "        if self.smoothing == 0:\n",
    "            return F.nll_loss(log_pred, target, ignore_index=self.ignore_index, reduction=self.reduction)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Initialize the target distribution with smoothing value.\n",
    "            true_dist = torch.full_like(log_pred, self.smoothing / (pred.size(1) - 1))\n",
    "            # Set the confidence for the correct labels.\n",
    "            true_dist.scatter_(1, target.unsqueeze(1), self.confidence)\n",
    "            # Zero out the probabilities for the ignore_index targets.\n",
    "            mask = target.unsqueeze(1) == self.ignore_index\n",
    "            true_dist.masked_fill_(mask, 0)\n",
    "\n",
    "        # Compute the loss.\n",
    "        loss = -torch.sum(true_dist * log_pred, dim=-1)\n",
    "\n",
    "        # Apply the chosen reduction on non-ignored targets.\n",
    "        valid_loss = loss[target != self.ignore_index]\n",
    "        if self.reduction == \"mean\":\n",
    "            loss = valid_loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            loss = valid_loss.sum()\n",
    "        else:\n",
    "            loss = valid_loss  # no reduction\n",
    "        return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa5e17ff-28eb-4b06-9b68-3bd9af90213a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import GPT2Tokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from tqdm import tqdm\n",
    "from torch.amp import GradScaler, autocast\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "#wikitext-103-raw-v1\n",
    "#wikitext-2-raw-v1\n",
    "def prepare_datasets(\n",
    "    dataset_variant=\"wikitext-103-raw-v1\", \n",
    "    batch_size=1000, \n",
    "    max_length=250, \n",
    "    num_proc=16, \n",
    "    use_cache=True\n",
    "):\n",
    "    from datasets import load_dataset\n",
    "    from transformers import GPT2Tokenizer\n",
    "    import os\n",
    "\n",
    "    # 1. Load the dataset variant (WikiText-2 or WikiText-3)\n",
    "    dataset = load_dataset(\"wikitext\", dataset_variant)\n",
    "\n",
    "    # 2. Setup tokenizer and add a pad token\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")  \n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    pad_id = tokenizer.pad_token_id\n",
    "\n",
    "    # 3. Define a tokenization function (with a simple text cleanup)\n",
    "    def tokenize(batch):\n",
    "        cleaned_text = [text.replace(\"@-@\", \"-\") for text in batch[\"text\"]]\n",
    "        return tokenizer(\n",
    "            cleaned_text,\n",
    "            max_length=max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            add_special_tokens=True\n",
    "        )\n",
    "    \n",
    "    # 4. Process datasets in parallel; use caching if desired\n",
    "    train_data = dataset[\"train\"].map(\n",
    "        tokenize,\n",
    "        batched=True,\n",
    "        batch_size=batch_size,\n",
    "        num_proc=num_proc,\n",
    "        remove_columns=[\"text\"],\n",
    "        load_from_cache_file=use_cache\n",
    "    )\n",
    "    \n",
    "    val_data = dataset[\"validation\"].map(\n",
    "        tokenize,\n",
    "        batched=True,\n",
    "        batch_size=batch_size,\n",
    "        num_proc=num_proc,\n",
    "        remove_columns=[\"text\"],\n",
    "        load_from_cache_file=use_cache\n",
    "    )\n",
    "\n",
    "    # 5. Filter out sequences that are entirely padding\n",
    "    train_data = train_data.filter(lambda x: any(token != pad_id for token in x[\"input_ids\"]))\n",
    "    val_data = val_data.filter(lambda x: any(token != pad_id for token in x[\"input_ids\"]))\n",
    "\n",
    "    print(f\"Train dataset length: {len(train_data)}\")\n",
    "    print(f\"Val dataset length: {len(val_data)}\")\n",
    "\n",
    "    # 6. Count tokens efficiently\n",
    "    total_train_tokens = sum(map(len, train_data[\"input_ids\"]))\n",
    "    total_val_tokens = sum(map(len, val_data[\"input_ids\"]))\n",
    "    print(f\"Total tokens in train dataset: {total_train_tokens}\")\n",
    "    print(f\"Total tokens in validation dataset: {total_val_tokens}\")\n",
    "    print(f\"Total tokens in dataset: {total_train_tokens + total_val_tokens}\")\n",
    "\n",
    "    return train_data, val_data, tokenizer\n",
    "\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Each item in the batch is {\"input_ids\": [...list of ints...]}\n",
    "    input_ids_list = [item[\"input_ids\"] for item in batch]\n",
    "    # Convert each list of ints to a 1D tensor\n",
    "    input_ids_tensors = [torch.tensor(ids, dtype=torch.long) for ids in input_ids_list]\n",
    "    # Pad/stack them if needed\n",
    "    input_ids = torch.stack(input_ids_tensors, dim=0)  # shape: [batch_size, seq_len]\n",
    "\n",
    "    # Return shifted inputs/targets\n",
    "    return input_ids[:, :-1], input_ids[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52f4ef0e-5733-4b5c-8a10-e0c174f9488d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def train_batch(model, inputs, targets, optimizer, device):\\n    inputs, targets = inputs.to(device), targets.to(device)\\n\\n    optimizer.zero_grad()\\n    logits = model(inputs)  # full float32\\n    loss = F.cross_entropy(\\n        logits.view(-1, logits.size(-1)),\\n        targets.reshape(-1),\\n        ignore_index=tokenizer.pad_token_id\\n    )\\n    loss.backward()\\n    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\\n    optimizer.step()\\n\\n\\n    return loss.item()\\n\\n    print(f\"Batch loss (no AMP): {loss.item():.4f}\")'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "def initialize_training(model, tokenizer):\n",
    "    device = torch.device(\"cuda\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-3, weight_decay=.01, foreach=True)\n",
    "    \n",
    "    # Mixed precision scaler\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    # Enable cudnn benchmarking\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    return model, optimizer, scaler, device\n",
    "\n",
    "# ====================\n",
    "# 4. TRAINING LOOP\n",
    "# ====================\n",
    "# Cell 4: Corrected Training Loop\n",
    "def train_batch(model, inputs, targets, optimizer, scaler, device, pad_id):\n",
    "    \"\"\"Process a single batch\"\"\"\n",
    "    # Use non_blocking transfers if possible\n",
    "    inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "    \n",
    "    with torch.amp.autocast('cuda'):\n",
    "        outputs = model(inputs)\n",
    "        loss = F.cross_entropy(\n",
    "            outputs.view(-1, outputs.size(-1)),\n",
    "            targets.view(-1),\n",
    "            ignore_index=pad_id\n",
    "        )\n",
    "    \n",
    "    # Backward pass with scaled loss\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.unscale_(optimizer)\n",
    "    \n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    \n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # Simple NaN check on scalar loss\n",
    "    if torch.isnan(loss):\n",
    "        print(\"NaN loss detected!\")\n",
    "        \n",
    "    return loss.item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6304ee9-3467-484c-97b4-602137fb3d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.amp import autocast  # Updated import\n",
    "\n",
    "def top_filtering(logits, top_k=0, top_p=1.0, filter_value=-float('Inf')):\n",
    "    \"\"\"Apply top-k and nucleus (top-p) filtering to logits.\n",
    "    \n",
    "    Args:\n",
    "        logits (torch.Tensor): Logits distribution of shape (vocab_size,).\n",
    "        top_k (int): Keep only top k tokens with highest probability.\n",
    "        top_p (float): Keep the smallest set of tokens with cumulative probability >= top_p.\n",
    "        filter_value (float): Logits to assign to filtered tokens.\n",
    "    Returns:\n",
    "        torch.Tensor: Filtered logits.\n",
    "    \"\"\"\n",
    "    # Ensure logits is a 1D tensor\n",
    "    assert logits.dim() == 1\n",
    "\n",
    "    # Top-k filtering\n",
    "    if top_k > 0:\n",
    "        top_k = min(top_k, logits.size(-1))\n",
    "        values, _ = torch.topk(logits, top_k)\n",
    "        min_value = values[-1]\n",
    "        logits[logits < min_value] = filter_value\n",
    "\n",
    "    # Top-p (nucleus) filtering\n",
    "    if top_p < 1.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "        \n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        \n",
    "        # Shift the indices to ensure at least one token is kept\n",
    "        sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()\n",
    "        sorted_indices_to_remove[0] = 0\n",
    "        \n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "        \n",
    "    return logits\n",
    "\n",
    "def generate_text_strict(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    device,\n",
    "    prompt=\"Attempts have been made...\",\n",
    "    max_length=50,\n",
    "    temperature=0.7,\n",
    "    top_k=40,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.2,\n",
    "    no_repeat_ngram_size=3\n",
    "):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "            # Apply temperature\n",
    "            logits = outputs[:, -1, :] / temperature\n",
    "            logits = logits.squeeze(0)\n",
    "            \n",
    "            # Repetition penalty\n",
    "            if repetition_penalty != 1.0:\n",
    "                logits = apply_repetition_penalty(logits, input_ids[0], penalty=repetition_penalty)\n",
    "\n",
    "            # No-repeat n-gram\n",
    "            if no_repeat_ngram_size > 0:\n",
    "                logits = no_repeat_ngram(logits, input_ids, n=no_repeat_ngram_size)\n",
    "\n",
    "            # Top-k + top-p\n",
    "            filtered_logits = top_filtering(logits, top_k=top_k, top_p=top_p)\n",
    "            \n",
    "            # Sample\n",
    "            probs = F.softmax(filtered_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1).unsqueeze(0)\n",
    "        \n",
    "        input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "        \n",
    "        # Stop if EOS\n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "        if input_ids.shape[-1] >= max_length:\n",
    "            break\n",
    "    \n",
    "    return tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def apply_repetition_penalty(logits, input_ids, penalty=2.5):\n",
    "    \"\"\"\n",
    "    Applies a repetition penalty by down-weighting the logits\n",
    "    of any previously generated token IDs. The idea is to reduce\n",
    "    the likelihood of repeating the exact same token if it appears\n",
    "    in input_ids.\n",
    "\n",
    "    Args:\n",
    "        logits (torch.Tensor): The predicted logits of shape (vocab_size,).\n",
    "        input_ids (torch.Tensor): The sequence of generated tokens so far\n",
    "                                  (shape (sequence_length,)).\n",
    "        penalty (float): The factor by which to down-weight repeated tokens.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Modified logits with repetition penalty applied.\n",
    "    \"\"\"\n",
    "    # For each unique token ID in input_ids, divide the logit by penalty\n",
    "    for token_id in set(input_ids.tolist()):\n",
    "        logits[token_id] /= penalty\n",
    "\n",
    "    return logits\n",
    "\n",
    "def no_repeat_ngram(logits, input_ids, n=2):\n",
    "    if input_ids.shape[-1] >= n:\n",
    "        # get last (n-1) tokens\n",
    "        recent_ngram = tuple(input_ids[0, -n+1:].tolist())\n",
    "        # find all possible (n-1) + next_token combos in input_ids\n",
    "        for i in range(input_ids.shape[-1] - n + 1):\n",
    "            # if you find the same n-1 tokens\n",
    "            if tuple(input_ids[0, i:i+n-1].tolist()) == recent_ngram:\n",
    "                # forbid the next token from that occurrence\n",
    "                forbidden_token = input_ids[0, i+n-1].item()\n",
    "                logits[forbidden_token] = -float('Inf')\n",
    "    return logits\n",
    "\n",
    "def validate(model, val_loader, device, pad_id):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            # Use non_blocking transfers if data is in pinned memory\n",
    "            inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "            \n",
    "            with torch.amp.autocast('cuda'):\n",
    "                outputs = model(inputs)\n",
    "                loss = F.cross_entropy(\n",
    "                    outputs.view(-1, outputs.size(-1)),\n",
    "                    targets.view(-1),\n",
    "                    ignore_index=pad_id\n",
    "                )\n",
    "            \n",
    "            batch_size = inputs.size(0)\n",
    "            total_loss += loss.item() * batch_size\n",
    "            total_samples += batch_size\n",
    "            \n",
    "    return total_loss / total_samples if total_samples > 0 else float('inf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "048a2a18-035a-459f-b41f-1a54f3e1eece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "\n",
    "# Configuration (easily adjustable)\n",
    "CONFIG = {\n",
    "    \"batch_size\": 64,      # RTX 4090 can handle larger batches\n",
    "    \"num_epochs\": 10,\n",
    "    \"eval_interval\": 2,    # Generate text every N epochs\n",
    "    \"max_seq_len\": 250,\n",
    "    \"temp\": 0.9,           # Generation temperature\n",
    "    \"top_k\": 20\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b61b6eb0-3f98-4522-9c9f-c428838d17b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and tokenizing data...\n",
      "Train dataset length: 1165029\n",
      "Val dataset length: 2461\n",
      "Total tokens in train dataset: 291257250\n",
      "Total tokens in validation dataset: 615250\n",
      "Total tokens in dataset: 291872500\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Prepare Data\n",
    "print(\"Loading and tokenizing data...\")\n",
    "train_data, val_data, tokenizer = prepare_datasets()\n",
    "print(any(all(token_id == tokenizer.pad_token_id for token_id in seq) for seq in train_data[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4aa77187-067e-4c7f-969f-bbe1679b86ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out all-pad sequences\n",
    "pad_id = tokenizer.pad_token_id\n",
    "def remove_all_pad(example):\n",
    "    ids = example[\"input_ids\"]\n",
    "    return any(token_id != pad_id for token_id in ids)\n",
    "\n",
    "train_data = train_data.filter(remove_all_pad)\n",
    "val_data   = val_data.filter(remove_all_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25a040f9-0c6f-4bd8-a7c3-3ee068069c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_data, \n",
    "    batch_size=64, \n",
    "    shuffle=True, \n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_data, \n",
    "    batch_size=64, \n",
    "    shuffle=False, \n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f747e2d2-2a1e-4647-a5a6-9a61b5e79fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing model...\n",
      "Model parameters: 44.7M\n",
      "Training on: cuda\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Initialize Model\n",
    "print(\"\\nInitializing model...\")\n",
    "model = TransformerLM(\n",
    "    vocab_size=len(tokenizer),\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    embed_dim=512,      \n",
    "    num_heads=8,        \n",
    "    num_layers=6,       \n",
    "    ff_dim=2056,        \n",
    "    dropout_rate=0.1,   \n",
    "    max_seq_len=250,\n",
    "    drop_path_rate=0.0,\n",
    "    sparse_window_size=25,\n",
    "    causal_heads = 4\n",
    ")\n",
    "\n",
    "model, optimizer, scaler, device = initialize_training(model, tokenizer)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters())/1e6:.1f}M\")\n",
    "print(f\"Training on: {device}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fae47a7-8541-4706-847b-9cf491c498ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|████████████████████████████████████████████████████████████████| 18204/18204 [37:00<00:00,  8.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss = 4.2359\n",
      "Epoch 1: Validation Loss = 3.8144, Perplexity = 45.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|████████████████████████████████████████████████████████████████| 18204/18204 [36:22<00:00,  8.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Training Loss = 3.8551\n",
      "Epoch 2: Validation Loss = 3.6919, Perplexity = 40.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|████████████████████████████████████████████████████████████████| 18204/18204 [36:23<00:00,  8.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Training Loss = 3.7656\n",
      "Epoch 3: Validation Loss = 3.6422, Perplexity = 38.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|████████████████████████████████████████████████████████████████| 18204/18204 [36:23<00:00,  8.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Training Loss = 3.7179\n",
      "Epoch 4: Validation Loss = 3.5983, Perplexity = 36.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|████████████████████████████████████████████████████████████████| 18204/18204 [36:23<00:00,  8.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Training Loss = 3.6873\n",
      "Epoch 5: Validation Loss = 3.5823, Perplexity = 35.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|████████████████████████████████████████████████████████████████| 18204/18204 [36:24<00:00,  8.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Training Loss = 3.6660\n",
      "Epoch 6: Validation Loss = 3.5684, Perplexity = 35.46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|████████████████████████████████████████████████████████████████| 18204/18204 [36:23<00:00,  8.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Training Loss = 3.6500\n",
      "Epoch 7: Validation Loss = 3.5540, Perplexity = 34.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|████████████████████████████████████████████████████████████████| 18204/18204 [36:24<00:00,  8.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Training Loss = 3.6378\n",
      "Epoch 8: Validation Loss = 3.5375, Perplexity = 34.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|████████████████████████████████████████████████████████████████| 18204/18204 [36:24<00:00,  8.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Training Loss = 3.6275\n",
      "Epoch 9: Validation Loss = 3.5468, Perplexity = 34.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|███████████████████████████████████████████████████████████████| 18204/18204 [36:24<00:00,  8.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Training Loss = 3.6191\n",
      "Epoch 10: Validation Loss = 3.5321, Perplexity = 34.20\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cell 4: Training Loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "num_epochs = CONFIG[\"num_epochs\"]\n",
    "\n",
    "label_smoothing = 0.000 # Adjust as needed\n",
    "ignore_index = tokenizer.pad_token_id  # Ensure this matches your padding token ID\n",
    "\n",
    "loss_fn = LabelSmoothingCrossEntropy(\n",
    "    ignore_index=ignore_index,\n",
    "    smoothing=label_smoothing\n",
    ")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for inputs, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        # Use non_blocking transfers if using pinned memory\n",
    "        inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "        \n",
    "        # Zero gradients once per iteration, more memory efficient\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        with torch.amp.autocast('cuda'):\n",
    "            outputs = model(inputs)  # [batch_size, seq_len, vocab_size]\n",
    "            # Flatten outputs and targets for loss computation\n",
    "            outputs = outputs.view(-1, outputs.size(-1))  # [batch_size * seq_len, vocab_size]\n",
    "            targets = targets.view(-1)  # [batch_size * seq_len]\n",
    "            loss = loss_fn(outputs, targets)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.8)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = epoch_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    print(f\"Epoch {epoch+1}: Training Loss = {avg_train_loss:.4f}\")\n",
    "    \n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                outputs = model(inputs)\n",
    "                outputs = outputs.view(-1, outputs.size(-1))\n",
    "                targets = targets.view(-1)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    perplexity = math.exp(avg_val_loss)\n",
    "    print(f\"Epoch {epoch+1}: Validation Loss = {avg_val_loss:.4f}, Perplexity = {perplexity:.2f}\")\n",
    "\n",
    "# Cell 5: Save Model (optional)\n",
    "print(\"Training complete!\")\n",
    "# model.save_pretrained(\"transformer_lm\")  # Uncomment to save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "2646d543-74d5-4324-ab6c-82b23f3cd508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: The United States is best known for its political and economic importance , which has helped sustain the world . It was an important step in the development of the modern economy and has been a major factor in the growth of the Soviet Union since World War II . \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_text = \"The United States is best known for\"\n",
    "generated = generate_text_strict(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    prompt=prompt_text,\n",
    "    temperature=.75,\n",
    "    top_k=100,\n",
    "    top_p=.90,\n",
    "    repetition_penalty=1.2,\n",
    "    no_repeat_ngram_size=1\n",
    ")\n",
    "print(\"Generated:\", generated)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4a1557-34c4-4604-941d-1f876cb75ee9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275a9000-7193-4533-bb08-78063c733dfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
